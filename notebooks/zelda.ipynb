{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN79dDC+aHpq3So44PVbaHB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "dn7pabGdIpOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/modAL-python/modAL.git\n",
        "!pip install ipdb"
      ],
      "metadata": {
        "id": "vCN-3R9jF5BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jd4tZnsr5Pu"
      },
      "outputs": [],
      "source": [
        "#@title import libraries\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pdb\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling\n",
        "from modAL.uncertainty import margin_sampling\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import Callback\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "8WTzgIqpI-OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value(input):\n",
        "  #51  53  56  62  66  70  80  (85  86  87  88  89  90  91  92  93  94 95  96  98  99 100 101 104 105 106 109 110 111 112 113 114 117]\n",
        "  if input in range(51,53): #empty\n",
        "    return 1\n",
        "  elif input in range(53,66): #enemy\n",
        "    return 2\n",
        "  elif input in range(66,80): #door\n",
        "    return 3\n",
        "  elif input in range (80,86): #key\n",
        "    return 4\n",
        "  elif input in range (88,92): #player\n",
        "    return 6\n",
        "  elif input in range (86,88) or (92,120): #wall\n",
        "    return 5\n"
      ],
      "metadata": {
        "id": "_4uuEmVRml3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Resources"
      ],
      "metadata": {
        "id": "rB6ZBh9yIy1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "# Define the border size\n",
        "map_border = 60\n",
        "tile_border = 0.5\n",
        "# Define the tile size\n",
        "tile_width = 60\n",
        "tile_height = 60\n",
        "distinct_values = [51,53,56,62,66,80,86,90,106]\n",
        "#51 : empty, 53: enemy,bat, 56:enemy,scorpions, 62: enemy,spider, 66:door, 80:key, 86: wall, 90:player, 106: wall\n",
        "#51 : empty, 53,56,62: enemy, 66:door, 80:key, 86,106: wall, 90:player\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb0Y_eLoNSEO",
        "outputId": "55f45f99-90c8-4fb1-f52c-6c34eb387b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title mounting playbale files\n",
        "playable_levels = []\n",
        "playable_levels_labels = []\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/zelda/playable'):\n",
        "    # Check if the file is an image\n",
        "    if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
        "        # Open the image file\n",
        "        img = Image.open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/zelda/playable', filename))\n",
        "        tile_ints = []\n",
        "        # Iterate over each tile in the tile map\n",
        "        for y in range(0, 7):\n",
        "          for x in range(0, 11):\n",
        "            # Calculate the coordinates of the current tile in the tile map\n",
        "            tile_x1 = map_border + x * (tile_width + 2*tile_border)\n",
        "            tile_y1 = map_border + y * (tile_height + 2*tile_border)\n",
        "            tile_x2 = tile_x1 + tile_width + 2*tile_border\n",
        "            tile_y2 = tile_y1 + tile_height + 2*tile_border\n",
        "            # Crop the border of the current tile\n",
        "            tile_cropped = img.crop((tile_x1, tile_y1, tile_x2, tile_y2))\n",
        "            # Convert the cropped tile image to grayscale and get the pixel values\n",
        "            tile_gray = tile_cropped.convert('L')\n",
        "            tile_pixels = list(tile_gray.getdata())\n",
        "            # Convert the pixel values to an integer value\n",
        "            tile_int = int(sum(tile_pixels) / len(tile_pixels))\n",
        "            tile_ints.append(get_value(tile_int))\n",
        "        if len(tile_ints) == 77:\n",
        "          arr = np.array(tile_ints)\n",
        "          # Reshape the array into a 2D array of size 7x11\n",
        "          arr = np.array(tile_ints).reshape((7, 11))\n",
        "          distinc = [1,2,3,4,5,6]\n",
        "          matrix_dict = {val: (arr == val).astype(int) for val in distinc}\n",
        "          for key in [1,2,3,4,5,6]:\n",
        "            if key not in matrix_dict:\n",
        "              matrix_dict[key] = np.zeros((7,11))\n",
        "\n",
        "          # Convert the dictionary of counts to a NumPy array\n",
        "          matrix_array = np.array([matrix_dict[val] for val in distinc])\n",
        "          playable_levels.append(matrix_array)\n",
        "          playable_levels_labels.append(1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S2tQdQsb_Y7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title mounting unplaybale files\n",
        "unplayable_levels = []\n",
        "unplayable_levels_labels = []\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/zelda/unplayable'):\n",
        "    # Check if the file is an image\n",
        "    if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
        "        # Open the image file\n",
        "        img = Image.open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/zelda/unplayable', filename))\n",
        "        tile_ints = []\n",
        "        # Iterate over each tile in the tile map\n",
        "        for y in range(0, 7):\n",
        "          for x in range(0, 11):\n",
        "            # Calculate the coordinates of the current tile in the tile map\n",
        "            tile_x1 = map_border + x * (tile_width + 2*tile_border)\n",
        "            tile_y1 = map_border + y * (tile_height + 2*tile_border)\n",
        "            tile_x2 = tile_x1 + tile_width + 2*tile_border\n",
        "            tile_y2 = tile_y1 + tile_height + 2*tile_border\n",
        "            # Crop the border of the current tile\n",
        "            tile_cropped = img.crop((tile_x1, tile_y1, tile_x2, tile_y2))\n",
        "            # Convert the cropped tile image to grayscale and get the pixel values\n",
        "            tile_gray = tile_cropped.convert('L')\n",
        "            tile_pixels = list(tile_gray.getdata())\n",
        "            # Convert the pixel values to an integer value\n",
        "            tile_int = int(sum(tile_pixels) / len(tile_pixels))\n",
        "            tile_int = int(sum(tile_pixels) / len(tile_pixels))\n",
        "            tile_ints.append(get_value(tile_int))\n",
        "        if len(tile_ints) == 77:\n",
        "          arr = np.array(tile_ints)\n",
        "          # Reshape the array into a 2D array of size 7x11\n",
        "          arr = np.array(tile_ints).reshape((7, 11))\n",
        "          distinc = [1,2,3,4,5,6]\n",
        "          matrix_dict = {val: (arr == val).astype(int) for val in distinc}\n",
        "          for key in [1,2,3,4,5,6]:\n",
        "            if key not in matrix_dict:\n",
        "              matrix_dict[key] = np.zeros((7,11))\n",
        "\n",
        "          # Convert the dictionary of counts to a NumPy array\n",
        "          matrix_array = np.array([matrix_dict[val] for val in distinc])\n",
        "          unplayable_levels.append(matrix_array)\n",
        "          unplayable_levels_labels.append(0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PGLM7srgNHzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title balance the dataset\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "playable_idx = np.random.choice(range(len(playable_levels)), size=5, replace=False)\n",
        "unplayable_idx = np.random.choice(range(len(unplayable_levels)), size=5, replace=False)\n",
        "initial_x = []\n",
        "initial_y = []\n",
        "\n",
        "for i in playable_idx:\n",
        "  initial_x.append(playable_levels[i])\n",
        "  initial_y.append(playable_levels_labels[i])\n",
        "\n",
        "for i in unplayable_idx:\n",
        "  initial_x.append(unplayable_levels[i])\n",
        "  initial_y.append(unplayable_levels_labels[i])\n",
        "\n",
        "\n",
        "\n",
        "levels = []\n",
        "levels.extend(playable_levels)\n",
        "levels.extend(unplayable_levels)\n",
        "levels = np.array(levels)\n",
        "print(levels.shape)\n",
        "\n",
        "labels = []\n",
        "labels.extend(playable_levels_labels)\n",
        "labels.extend(unplayable_levels_labels)\n",
        "labels = np.array(labels)\n",
        "print(labels.shape)\n",
        "\n",
        "initial_x = np.array(initial_x)\n",
        "print(initial_x.shape)\n",
        "initial_y = np.array(initial_y)\n",
        "print(initial_y.shape)\n",
        "\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))"
      ],
      "metadata": {
        "id": "U1aBAh71OwTX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "H1Zy6R-LJa_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNet18Custom(nn.Module):\n",
        "    def __init__(self, num_channels=6, num_classes=2):\n",
        "        super(ResNet18Custom, self).__init__()\n",
        "        self.resnet = models.resnet18()\n",
        "        self.resnet.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(512, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        #x = self.sigmoid(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "        #x = self.resnet(x)\n",
        "        #x = self.resnet.conv1(x)\n",
        "        #x = self.resnet.fc(x)\n",
        "        #return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(6, 16, kernel_size=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 32)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "frMXx7j-FgR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passive Learner"
      ],
      "metadata": {
        "id": "URGc4HqYJf_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import pickle\n",
        "import ipdb\n",
        "\n",
        "def encode_labels(labels):\n",
        "    if not isinstance(labels, torch.Tensor) or labels.dtype != torch.float32:\n",
        "        raise ValueError(\"Input must be a PyTorch tensor of float32 dtype.\")\n",
        "    if labels.ndim != 1 or not torch.all(torch.logical_or(labels == 0, labels == 1)):\n",
        "        raise ValueError(\"Input must be a 1D PyTorch tensor of 0s and 1s.\")\n",
        "    encoded_labels = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
        "    encoded_labels[torch.where(labels == 0)[0], 0] = 1\n",
        "    encoded_labels[torch.where(labels == 1)[0], 1] = 1\n",
        "    return encoded_labels\n",
        "\n",
        "X_tensor = torch.FloatTensor(levels)\n",
        "y_tensor = torch.FloatTensor(labels)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(train_data))\n",
        "\n",
        "num_folds = 5\n",
        "num_epochs = 50\n",
        "weight_decay=1e-3 # reasonale 1e-4 to 1e-2.\n",
        "learning_rate = 1e-2 # reasonable 1e-4 and 0.1\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "all_fold_acc_history = []\n",
        "start_time = time.time()\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(test_data)):\n",
        "    model = Model2()\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "    train_fold_data = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_fold_data = torch.utils.data.Subset(test_data, val_indices)\n",
        "    train_fold_loader = torch.utils.data.DataLoader(train_fold_data, batch_size=32, shuffle=True)\n",
        "    val_fold_loader = torch.utils.data.DataLoader(val_fold_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    fold_acc_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for inputs, labels in train_fold_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            #ipdb.set_trace()\n",
        "            loss = criterion(outputs, encode_labels(labels))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(val_fold_loader)\n",
        "        print(f\"Validation fold {fold + 1}, epoch {epoch + 1}: train_loss = {train_loss:.2f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            true_labels = []\n",
        "            pred_labels = []\n",
        "            for inputs, labels in val_fold_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, encode_labels(labels))\n",
        "                val_loss += loss.item()\n",
        "                y_pred = np.argmax(outputs, axis=1)\n",
        "                true_labels += labels.tolist()\n",
        "                pred_labels += y_pred.round().tolist()\n",
        "            val_loss /= len(val_fold_loader)\n",
        "\n",
        "        val_acc = accuracy_score(true_labels, pred_labels)\n",
        "        fold_acc_history.append(val_acc)\n",
        "        print(f\"Fold {fold + 1}: val_loss = {val_loss:.2f}, val_acc = {val_acc:.2f}\")\n",
        "    all_fold_acc_history.append(fold_acc_history)\n",
        "    all_true_labels += true_labels\n",
        "    all_pred_labels += pred_labels\n",
        "\n",
        "# Compute the overall confusion matrix and accuracy\n",
        "overall_cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "overall_acc = accuracy_score(all_true_labels, all_pred_labels)\n",
        "\n",
        "# Print the overall confusion matrix and accuracy\n",
        "print(f\"Overall confusion matrix:\\n{overall_cm}\")\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")\n",
        "\n",
        "# Compute the average accuracy over all folds for each epoch\n",
        "mean_acc_history = [sum([fold_acc_history[i] for fold_acc_history in all_fold_acc_history])/num_folds for i in range(len(all_fold_acc_history[0]))]\n",
        "\n",
        "# Plot the average accuracy over all folds over time\n",
        "plt.plot(mean_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Time')\n",
        "plt.show()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time of: {execution_time} seconds\")\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/passive/model_'+timestamp+'_.h5')\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'folds' : num_folds, 'epochs' : num_epochs,\n",
        "    'weight_decay' : weight_decay, 'learning_rate': learning_rate,\n",
        "    'execution_time' : execution_time}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/passive/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/passive/parameters-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(\n",
        "        {'all_fold_acc_history' : all_fold_acc_history,'confusion' : overall_cm}\n",
        "        , handle)"
      ],
      "metadata": {
        "id": "50_j9lB7KOvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learner"
      ],
      "metadata": {
        "id": "snGW9W1BJlQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pdb\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "#random_query_strategy\n",
        "#margin_sampling\n",
        "#entropy_sampling\n",
        "#uncertainty_sampling\n",
        "\n",
        "def random_query_strategy(classifier, X, n_instances=1):\n",
        "    indices = list(range(len(X)))\n",
        "    random.shuffle(indices)\n",
        "    return indices[:n_instances]\n",
        "\n",
        "def train_active_learner(strategy, n_queries, n_instances, max_epochs, fraction):\n",
        "    num_folds = 5\n",
        "    num_rounds = 5\n",
        "    all_fold_acc_history = []\n",
        "    all_fold_false_positives = []\n",
        "    all_fold_false_negatives = []\n",
        "    # initial training dataset\n",
        "    X_tensor = torch.FloatTensor(levels)\n",
        "    y_tensor = torch.LongTensor(labels)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "    X , y = next(iter(dataset_loader))\n",
        "    X = X.detach().cpu().numpy()\n",
        "    y = y.detach().cpu().numpy()\n",
        "    #\n",
        "    X_ini = torch.FloatTensor(initial_x)\n",
        "    y_ini = torch.LongTensor(initial_y)\n",
        "    ini_dataset = torch.utils.data.TensorDataset(torch.tensor(X_ini).to(device), torch.tensor(y_ini).to(device))\n",
        "    ini_loader = torch.utils.data.DataLoader(ini_dataset, batch_size=len(ini_dataset))\n",
        "    X_0 , y_0 = next(iter(ini_loader))\n",
        "    X_0 = X_0.detach().cpu().numpy()\n",
        "    y_0 = y_0.detach().cpu().numpy()\n",
        "\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "        X_train = X[train_indices]\n",
        "        X_test= X[val_indices]\n",
        "        y_train = y[train_indices]\n",
        "        y_test = y[val_indices]\n",
        "\n",
        "        print(f\"Fold no {fold+1}\")\n",
        "        # 5 trials\n",
        "        rounds_accuracies = []\n",
        "        rounds_fps = []\n",
        "        rounds_fns = []\n",
        "        for round in range (0,num_rounds):\n",
        "            print(f\"Fold no {fold+1} . Round no {round+1}\")\n",
        "            this_round_accuracies = []\n",
        "            this_round_fps = 0\n",
        "            this_round_fns = 0\n",
        "            classifier = NeuralNetClassifier(Model, criterion=nn.CrossEntropyLoss,\n",
        "                                 optimizer=torch.optim.Adam, optimizer__weight_decay=0.001,\n",
        "                                 max_epochs = max_epochs, train_split=None,\n",
        "                                 verbose=0, device=device, warm_start = True,)\n",
        "            learner = ActiveLearner(estimator=classifier,X_training=X_0, y_training=y_0,query_strategy=strategy)\n",
        "            y_pred = learner.predict(X_test)\n",
        "            ini_acc = accuracy_score(y_test, y_pred)\n",
        "            print(ini_acc)\n",
        "            fold_acc_history = []\n",
        "            for idx in range(n_queries):\n",
        "                random_indices = np.random.choice(range(len(X_train)), size=math.floor(fraction*len(X_train)), replace=False)\n",
        "                query_idx, query_instance = learner.query(X_train[random_indices], n_instances=n_instances)\n",
        "                x_query = X_train[query_idx]\n",
        "                y_query = y_train[query_idx]\n",
        "                learner.teach(X=x_query, y=y_query)\n",
        "                y_pred = learner.predict(X_test)\n",
        "\n",
        "                true_labels = y_test.tolist()\n",
        "                pred_labels = y_pred.tolist()\n",
        "                for kk in range(len(true_labels)):\n",
        "                    if (pred_labels[kk] == 1) & (true_labels[kk] == 0):\n",
        "                        this_round_fps += 1\n",
        "                    if (pred_labels[kk] == 0) & (true_labels[kk] == 1):\n",
        "                        this_round_fns += 1\n",
        "\n",
        "                val_acc = accuracy_score(y_test, y_pred)\n",
        "                this_round_accuracies.append(val_acc)\n",
        "\n",
        "                X_train = np.delete(X_train, query_idx, axis=0)\n",
        "                y_train = np.delete(y_train, query_idx, axis=0)\n",
        "                print(f\"Fold no {fold+1} . Round no {round+1} . Query no {idx+1}: Acc = {val_acc:.2f}\")\n",
        "            rounds_accuracies.append(this_round_accuracies)\n",
        "            rounds_fps.append(this_round_fps)\n",
        "            rounds_fns.append(this_round_fns)\n",
        "        all_rounds_mean_accuracy = np.mean(rounds_accuracies, axis=0)\n",
        "        all_fold_acc_history.append(all_rounds_mean_accuracy)\n",
        "\n",
        "        all_rounds_fps = np.mean(rounds_fps)\n",
        "        all_rounds_fns = np.mean(rounds_fns)\n",
        "        all_fold_false_positives.append(all_rounds_fps)\n",
        "        all_fold_false_negatives.append(all_rounds_fns)\n",
        "\n",
        "        print(f\"Fold no {fold+1} . ALL ACC = {all_rounds_mean_accuracy}: total:{len(y_test)} : fps:{all_fold_false_positives} : nps:{all_fold_false_negatives}\")\n",
        "        timestamp = str(int(time.time()))\n",
        "        with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/model_fold'+str(fold+1)+'-'+timestamp+'.pickle', 'wb') as handle:\n",
        "            pickle.dump(classifier, handle)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return  classifier, all_fold_acc_history , execution_time, all_fold_false_positives, all_fold_false_negatives\n",
        "\n"
      ],
      "metadata": {
        "id": "Zg_HMEUurach"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Margin Sampling\n",
        "import pickle\n",
        "\n",
        "STRATEGY = margin_sampling\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 100\n",
        "MAX_EPOCHS = 50\n",
        "FRACTION = 0.5\n",
        "np.random.seed(75)\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "levels = []\n",
        "levels.extend(playable_levels)\n",
        "levels.extend(unplayable_levels)\n",
        "levels = np.array(levels)\n",
        "print(levels.shape)\n",
        "\n",
        "labels = []\n",
        "labels.extend(playable_levels_labels)\n",
        "labels.extend(unplayable_levels_labels)\n",
        "labels = np.array(labels)\n",
        "print(labels.shape)\n",
        "\n",
        "classifier_1, all_fold_acc_history_1, execution_time_1, fps_1,fns_1= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_1)\n",
        "print(fns_1)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(1, len(mean_accuracy_1) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "plt.errorbar(epochs_1, mean_accuracy_1, yerr=std_error_1, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_1, mean_accuracy_1, label='Average Accuracy')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_1, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'margin_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_1}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_1,'fps' : fps_1, 'fns': fns_1}, handle)\n"
      ],
      "metadata": {
        "id": "PcbDIE_GgXyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random Sampling\n",
        "STRATEGY = random_query_strategy\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 100\n",
        "MAX_EPOCHS = 50\n",
        "FRACTION = 0.5\n",
        "np.random.seed(30)  # Set the seed value to 42\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "levels = []\n",
        "levels.extend(playable_levels)\n",
        "levels.extend(unplayable_levels)\n",
        "levels = np.array(levels)\n",
        "print(levels.shape)\n",
        "\n",
        "labels = []\n",
        "labels.extend(playable_levels_labels)\n",
        "labels.extend(unplayable_levels_labels)\n",
        "labels = np.array(labels)\n",
        "print(labels.shape)\n",
        "\n",
        "classifier_2, all_fold_acc_history_2, execution_time_2, fps_2,fns_2= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_2)\n",
        "print(fns_2)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(1, len(mean_accuracy_2) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.errorbar(epochs_2, mean_accuracy_2, yerr=std_error_2, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_2, mean_accuracy_2, label='Average Accuracy')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_2, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'random_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_2}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Zelda-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_2,'fps' : fps_2, 'fns': fns_2}, handle)"
      ],
      "metadata": {
        "id": "vw6t_zmAg-oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "FZt1zvLnJsbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(10, len(mean_accuracy_1) + 10)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(10, len(mean_accuracy_2) + 10)\n",
        "print(epochs_2)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.plot(epochs_1,  mean_accuracy_1, color='b', linestyle='-', label='Margin Sampling')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3)\n",
        "plt.plot(epochs_2,  mean_accuracy_2,color='r', linestyle='-', label='Random Sampling')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3)\n",
        "#plt.axhline(y=np.mean(all_fold_acc_history), color='g', linestyle='--', label='Passive Learner (2480 samples)')\n",
        "plt.ylim(0.45, 0.8)\n",
        "#plt.xlim(10, 410)\n",
        "#plt.xticks(np.arange(10,410, 50))\n",
        "plt.xlabel('#Samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Samples')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMvQGkEEET3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}