{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "Xcqw-5Kb04vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip install git+https://github.com/modAL-python/modAL.git"
      ],
      "metadata": {
        "id": "1mhxJ_dCNBYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOYJ45FRu0ok"
      },
      "outputs": [],
      "source": [
        "#@title import libraries\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pdb\n",
        "import modAL\n",
        "\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling\n",
        "from modAL.uncertainty import margin_sampling\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import Callback\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "z1Vjgvwe1Hw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsmg06l8u9bJ"
      },
      "outputs": [],
      "source": [
        "# Define the categories using a dictionary\n",
        "category_dict = {\"X\": \"ground\",\n",
        "                 \"S\": \"breakable\",\n",
        "                 \"-\": \"empty\",\n",
        "                 \"?\": \"question block\",\n",
        "                 \"Q\": \"question block\",\n",
        "                 \"E\": \"enemy\",\n",
        "                 \"<\": \"pipe\",\n",
        "                 \">\": \"pipe\",\n",
        "                 \"[\": \"pipe\",\n",
        "                 \"]\": \"pipe\",\n",
        "                 \"o\": \"coin\",\n",
        "                 \"B\": \"cannon\",\n",
        "                 \"b\": \"cannon\"}\n",
        "\n",
        "def merge_dict_keys(dict_in):\n",
        "    # merge tile categories that are the same\n",
        "    dict_out = {}\n",
        "    for key, value in dict_in.items():\n",
        "        if value not in dict_out:\n",
        "            dict_out[key] = value\n",
        "        else:\n",
        "            dict_out[key] += ', ' + value\n",
        "    dict_out = {v: k for k, v in dict_out.items()}\n",
        "    return {v: k for k, v in dict_out.items()}\n",
        "\n",
        "def create_matrix(string):\n",
        "    rows = string.split('\\n')\n",
        "    rows = [row for row in rows if row]\n",
        "    matrix = []\n",
        "    for row in rows:\n",
        "        columns = [char for char in row]\n",
        "        matrix.append(columns)\n",
        "    return matrix\n",
        "\n",
        "def replace_characters(matrix):\n",
        "    # Define the characters to replace and their replacements.\n",
        "    replacements = {'?': 'Q', '<': '[', '>': '[', ']': '[', 'b': 'B'}\n",
        "    new_matrix = []\n",
        "    for row in matrix:\n",
        "        new_row = []\n",
        "        for element in row:\n",
        "            if element in replacements:\n",
        "                new_row.append(replacements[element])\n",
        "            else:\n",
        "                new_row.append(element)\n",
        "        new_matrix.append(new_row)\n",
        "    return new_matrix\n",
        "\n",
        "def create_binary_matrices(matrix):\n",
        "    characters = {'X', 'S', '-', 'Q', 'E', '[', 'B','o'}\n",
        "    num_rows = len(matrix)\n",
        "    num_cols = len(matrix[0])\n",
        "    binary_matrices = {}\n",
        "    for character in characters:\n",
        "        binary_matrix = np.zeros((num_rows, num_cols))\n",
        "        for i in range(num_rows):\n",
        "            for j in range(num_cols):\n",
        "                if matrix[i][j] == character:\n",
        "                    binary_matrix[i][j] = 1\n",
        "        binary_matrices[character] = binary_matrix\n",
        "    return binary_matrices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_category_dict = merge_dict_keys(category_dict)\n",
        "print(merged_category_dict)"
      ],
      "metadata": {
        "id": "As393oFPXAgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Resources"
      ],
      "metadata": {
        "id": "og9OH_581CC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K6vHSClutaS"
      },
      "outputs": [],
      "source": [
        "# extracting labels\n",
        "drive.mount('/content/drive')\n",
        "gram_dict = {}\n",
        "map_dict = {}\n",
        "ngram_dict = {}\n",
        "\n",
        "# gram elite info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/mario/gram_elite_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    for filename in data['fitness'].keys():\n",
        "      gram_dict[filename] = data['fitness'][filename]\n",
        "print(\"gram_elite_info.json imported\")\n",
        "\n",
        "# map elite info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/mario/map_elites_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    for filename in data['fitness'].keys():\n",
        "      map_dict[filename] = data['fitness'][filename]\n",
        "print(\"map_elites_info.json imported\")\n",
        "\n",
        "# n gram info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/mario/ngram_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    for filename in data['fitness'].keys():\n",
        "      ngram_dict[filename] = data['fitness'][filename]\n",
        "print(\"ngram_info.json imported\")\n",
        "\n",
        "# extracting levels\n",
        "playable_level_strings = []\n",
        "unplayable_level_strings = []\n",
        "playable_level_labels = []\n",
        "unplayable_level_labels = []\n",
        "\n",
        "#gram elite\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/mario/gram elite levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/mario/gram elite levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in gram_dict:\n",
        "            if gram_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"gram elite levels imported\")\n",
        "\n",
        "#map elite\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/mario/map elites levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/mario/map elites levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in map_dict:\n",
        "            if map_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"map elite levels imported\")\n",
        "\n",
        "#n gram\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/mario/n gram levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/mario/n gram levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in ngram_dict:\n",
        "            if ngram_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"n gram levels imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofEzs_CPvHAw"
      },
      "outputs": [],
      "source": [
        "#@title balance the dataset\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "print(\"number of playable levels before balancing : %4d| number of unplayable levels before balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))\n",
        "\n",
        "# Reshape X to a 2D array\n",
        "#X_flat = np.reshape(levels, (levels.shape[0], -1))\n",
        "#oversampler = RandomUnderSampler()\n",
        "#y_flat = labels\n",
        "#X_resampled_flat, y_resampled_flat = oversampler.fit_resample(X_flat, y_flat)\n",
        "\n",
        "# Reshape the 2D array back to the original shape\n",
        "#X_resampled = np.reshape(X_resampled_flat, (-1, levels.shape[1], levels.shape[2], levels.shape[3]))\n",
        "#y_resampled = np.reshape(y_resampled_flat, (-1, 1))\n",
        "#y_resampled = np.repeat(y_resampled, 2, axis=1)\n",
        "\n",
        "#print(X_resampled.shape)\n",
        "#print(y_resampled.shape)\n",
        "#print(\"number of playable levels after balancing : %4d| number of unplayable levels after balancing : %4d|\" % (np.count_nonzero(y_resampled == 0), np.count_nonzero(y_resampled == 1)))\n",
        "\n",
        "# balance the dataset\n",
        "print(\"number of playable levels before balancing : %4d| number of unplayable levels before balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))\n",
        "n = len(unplayable_level_strings) - len(playable_level_strings)\n",
        "for i in range(n):\n",
        "    index = random.randint(0, len(unplayable_level_strings)-1)\n",
        "    unplayable_level_strings.pop(index)\n",
        "    unplayable_level_labels.pop(index)\n",
        "print(\"number of playable levels after balancing : %4d| number of unplayable levels after balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIojf2oMvEuY"
      },
      "outputs": [],
      "source": [
        "#@title convert level strings to one-hot matrices\n",
        "INITIAL_TRAINING_SIZE = 5\n",
        "print(len(playable_level_strings))\n",
        "print(len(unplayable_level_strings))\n",
        "level_strings = []\n",
        "level_strings.extend(playable_level_strings)\n",
        "level_strings.extend(unplayable_level_strings)\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "\n",
        "playable_idx = np.random.choice(range(len(playable_level_strings)), size=5, replace=False)\n",
        "unplayable_idx = np.random.choice(range(len(unplayable_level_strings)), size=5, replace=False)\n",
        "initial_x = []\n",
        "initial_y = []\n",
        "\n",
        "for i in playable_idx:\n",
        "  matrix = create_matrix(playable_level_strings[i])\n",
        "  replaced = replace_characters(matrix)\n",
        "  binary = create_binary_matrices(replaced)\n",
        "  initial_x.append(np.array(list(binary.values())))\n",
        "  initial_y.append(1)\n",
        "\n",
        "for i in playable_idx:\n",
        "  matrix = create_matrix(playable_level_strings[i])\n",
        "  replaced = replace_characters(matrix)\n",
        "  binary = create_binary_matrices(replaced)\n",
        "  initial_x.append(np.array(list(binary.values())))\n",
        "  initial_y.append(0)\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 8,14,25))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "qsz0ARnaRV16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDFBoZl4vI8e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, padding=0)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(640, 32)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        #x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "class Model3(nn.Module):\n",
        "    def __init__(self, num_channels=8, num_classes=2):\n",
        "        super(Model3, self).__init__()\n",
        "        self.resnet = models.resnet18()\n",
        "        self.resnet.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(512, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        #x = self.sigmoid(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "        #x = self.resnet(x)\n",
        "        #x = self.resnet.conv1(x)\n",
        "        #x = self.resnet.fc(x)\n",
        "        #return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import ipdb\n",
        "\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, input_dim = (8, 14, 25), label_size = 2, latent_dim= 32):\n",
        "        super(Model2, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear((input_dim[0] * input_dim[1] * input_dim[2]) + label_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 2)  # Adjust output size to 2, assuming 2 classes\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.cat((x, label), dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        output = self.softmax(x)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "CXtIA02yJe47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# passive learner with convolonutional layers"
      ],
      "metadata": {
        "id": "vmZJI-9h1U1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "def encode_labels(labels):\n",
        "    if not isinstance(labels, torch.Tensor) or labels.dtype != torch.float32:\n",
        "        raise ValueError(\"Input must be a PyTorch tensor of float32 dtype.\")\n",
        "    if labels.ndim != 1 or not torch.all(torch.logical_or(labels == 0, labels == 1)):\n",
        "        raise ValueError(\"Input must be a 1D PyTorch tensor of 0s and 1s.\")\n",
        "    encoded_labels = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
        "    encoded_labels[torch.where(labels == 0)[0], 0] = 1\n",
        "    encoded_labels[torch.where(labels == 1)[0], 1] = 1\n",
        "    return encoded_labels\n",
        "\n",
        "X_tensor = torch.FloatTensor(levels)\n",
        "y_tensor = torch.FloatTensor(labels)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(train_data))\n",
        "\n",
        "num_folds = 5\n",
        "num_epochs = 50\n",
        "weight_decay=0.001\n",
        "learning_rate = 1e-2\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "all_fold_acc_history = []\n",
        "start_time = time.time()\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(test_data)):\n",
        "    model = Model3()\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "    train_fold_data = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_fold_data = torch.utils.data.Subset(test_data, val_indices)\n",
        "    train_fold_loader = torch.utils.data.DataLoader(train_fold_data, batch_size=32, shuffle=True)\n",
        "    val_fold_loader = torch.utils.data.DataLoader(val_fold_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    fold_acc_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for inputs, labels in train_fold_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, encode_labels(labels))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(val_fold_loader)\n",
        "        print(f\"Validation fold {fold + 1}, epoch {epoch + 1}: train_loss = {train_loss:.2f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            true_labels = []\n",
        "            pred_labels = []\n",
        "            for inputs, labels in val_fold_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, encode_labels(labels))\n",
        "                val_loss += loss.item()\n",
        "                y_pred = np.argmax(outputs, axis=1)\n",
        "                true_labels += labels.tolist()\n",
        "                pred_labels += y_pred.round().tolist()\n",
        "            val_loss /= len(val_fold_loader)\n",
        "\n",
        "        val_acc = accuracy_score(true_labels, pred_labels)\n",
        "        fold_acc_history.append(val_acc)\n",
        "        print(f\"Fold {fold + 1}: val_loss = {val_loss:.2f}, val_acc = {val_acc:.2f}\")\n",
        "    all_fold_acc_history.append(fold_acc_history)\n",
        "    all_true_labels += true_labels\n",
        "    all_pred_labels += pred_labels\n",
        "\n",
        "# Compute the overall confusion matrix and accuracy\n",
        "overall_cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "overall_acc = accuracy_score(all_true_labels, all_pred_labels)\n",
        "\n",
        "# Print the overall confusion matrix and accuracy\n",
        "print(f\"Overall confusion matrix:\\n{overall_cm}\")\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")\n",
        "\n",
        "# Compute the average accuracy over all folds for each epoch\n",
        "mean_acc_history = [sum([fold_acc_history[i] for fold_acc_history in all_fold_acc_history])/num_folds for i in range(len(all_fold_acc_history[0]))]\n",
        "\n",
        "# Plot the average accuracy over all folds over time\n",
        "plt.plot(mean_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Time')\n",
        "plt.show()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time of: {execution_time} seconds\")\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/passive/model_'+timestamp+'_.h5')\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'folds' : num_folds, 'epochs' : num_epochs,\n",
        "    'weight_decay' : weight_decay, 'learning_rate': learning_rate,\n",
        "    'execution_time' : execution_time}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/passive/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/passive/parameters-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(\n",
        "        {'all_fold_acc_history' : all_fold_acc_history,'confusion' : overall_cm}\n",
        "        , handle)"
      ],
      "metadata": {
        "id": "0OE_DeiYsAGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# passive learner with fully connected layers"
      ],
      "metadata": {
        "id": "v1FaC_qsOAig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import pickle\n",
        "import torch.optim as optim\n",
        "def encode_labels(labels):\n",
        "    if not isinstance(labels, torch.Tensor) or labels.dtype != torch.float32:\n",
        "        raise ValueError(\"Input must be a PyTorch tensor of float32 dtype.\")\n",
        "    if labels.ndim != 1 or not torch.all(torch.logical_or(labels == 0, labels == 1)):\n",
        "        raise ValueError(\"Input must be a 1D PyTorch tensor of 0s and 1s.\")\n",
        "    encoded_labels = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
        "    encoded_labels[torch.where(labels == 0)[0], 0] = 1\n",
        "    encoded_labels[torch.where(labels == 1)[0], 1] = 1\n",
        "    return encoded_labels\n",
        "\n",
        "X_tensor = torch.FloatTensor(levels)\n",
        "y_tensor = torch.FloatTensor(labels)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(train_data))\n",
        "\n",
        "num_folds = 5\n",
        "num_epochs = 50\n",
        "weight_decay=0.001\n",
        "learning_rate = 1e-2\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "all_fold_acc_history = []\n",
        "start_time = time.time()\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(test_data)):\n",
        "    input_dim = (8, 14, 25)\n",
        "    label_size = 2\n",
        "    latent_dim = 32\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Create an instance of the model\n",
        "    model = Model2(input_dim, label_size, latent_dim)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_fold_data = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_fold_data = torch.utils.data.Subset(test_data, val_indices)\n",
        "    train_fold_loader = torch.utils.data.DataLoader(train_fold_data, batch_size=32, shuffle=True)\n",
        "    val_fold_loader = torch.utils.data.DataLoader(val_fold_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    fold_acc_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for inputs, labels in train_fold_loader:\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, encode_labels(labels))\n",
        "            #ipdb.set_trace()\n",
        "\n",
        "            loss = criterion(outputs, encode_labels(labels))\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(val_fold_loader)\n",
        "        print(f\"Validation fold {fold + 1}, epoch {epoch + 1}: train_loss = {train_loss:.2f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            for inputs, labels in val_fold_loader:\n",
        "                outputs = model(inputs, encode_labels(labels))\n",
        "                loss = criterion(outputs, encode_labels(labels))\n",
        "                val_loss += loss.item()\n",
        "                y_pred = np.argmax(outputs, axis=1)\n",
        "            val_loss /= len(val_fold_loader)\n",
        "\n",
        "        val_acc = accuracy_score(true_labels, pred_labels)\n",
        "        fold_acc_history.append(val_acc)\n",
        "        print(f\"Fold {fold + 1}: val_loss = {val_loss:.2f}, val_acc = {val_acc:.2f}\")\n",
        "    all_fold_acc_history.append(fold_acc_history)\n",
        "\n",
        "mean_acc_history = [sum([fold_acc_history[i] for fold_acc_history in all_fold_acc_history])/num_folds for i in range(len(all_fold_acc_history[0]))]\n",
        "\n",
        "# Plot the average accuracy over all folds over time\n",
        "plt.plot(mean_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Time')\n",
        "plt.show()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time of: {execution_time} seconds\")"
      ],
      "metadata": {
        "id": "5tYwqXSeJ8fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learner"
      ],
      "metadata": {
        "id": "RGCjrUD2Rd1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title active learner\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pdb\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "#random_query_strategy\n",
        "#margin_sampling\n",
        "#entropy_sampling\n",
        "#uncertainty_sampling\n",
        "\n",
        "def train_active_learner(strategy, n_queries, n_instances, max_epochs, fraction):\n",
        "    num_folds = 5\n",
        "    num_rounds = 5\n",
        "    all_fold_acc_history = []\n",
        "    all_fold_false_positives = []\n",
        "    all_fold_false_negatives = []\n",
        "    # initial training dataset\n",
        "    X_tensor = torch.FloatTensor(levels)\n",
        "    y_tensor = torch.LongTensor(labels)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "    X , y = next(iter(dataset_loader))\n",
        "    X = X.detach().cpu().numpy()\n",
        "    y = y.detach().cpu().numpy()\n",
        "    #\n",
        "    X_ini = torch.FloatTensor(initial_x)\n",
        "    y_ini = torch.LongTensor(initial_y)\n",
        "    ini_dataset = torch.utils.data.TensorDataset(torch.tensor(X_ini).to(device), torch.tensor(y_ini).to(device))\n",
        "    ini_loader = torch.utils.data.DataLoader(ini_dataset, batch_size=len(ini_dataset))\n",
        "    X_0 , y_0 = next(iter(ini_loader))\n",
        "    X_0 = X_0.detach().cpu().numpy()\n",
        "    y_0 = y_0.detach().cpu().numpy()\n",
        "\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "        X_train = X[train_indices]\n",
        "        X_test= X[val_indices]\n",
        "        y_train = y[train_indices]\n",
        "        y_test = y[val_indices]\n",
        "\n",
        "        print(f\"Fold no {fold+1}\")\n",
        "        # 5 trials\n",
        "        rounds_accuracies = []\n",
        "        rounds_fps = []\n",
        "        rounds_fns = []\n",
        "        for round in range (0,num_rounds):\n",
        "            print(f\"Fold no {fold+1} . Round no {round+1}\")\n",
        "            this_round_accuracies = []\n",
        "            this_round_fps = 0\n",
        "            this_round_fns = 0\n",
        "            classifier = NeuralNetClassifier(Model3, criterion=nn.CrossEntropyLoss,\n",
        "                                 optimizer=torch.optim.Adam, optimizer__weight_decay=0.001,\n",
        "                                 max_epochs = max_epochs, train_split=None,\n",
        "                                 verbose=0, device=device, warm_start = True,)\n",
        "            learner = ActiveLearner(estimator=classifier,X_training=X_0, y_training=y_0,query_strategy=strategy)\n",
        "            y_pred = learner.predict(X_test)\n",
        "            ini_acc = accuracy_score(y_test, y_pred)\n",
        "            print(ini_acc)\n",
        "            fold_acc_history = []\n",
        "            for idx in range(n_queries):\n",
        "                random_indices = np.random.choice(range(len(X_train)), size=math.floor(fraction*len(X_train)), replace=False)\n",
        "                query_idx, query_instance = learner.query(X_train[random_indices], n_instances=n_instances)\n",
        "                x_query = X_train[query_idx]\n",
        "                y_query = y_train[query_idx]\n",
        "                learner.teach(X=x_query, y=y_query)\n",
        "                y_pred = learner.predict(X_test)\n",
        "\n",
        "                true_labels = y_test.tolist()\n",
        "                pred_labels = y_pred.tolist()\n",
        "                for kk in range(len(true_labels)):\n",
        "                    if (pred_labels[kk] == 1) & (true_labels[kk] == 0):\n",
        "                        this_round_fps += 1\n",
        "                    if (pred_labels[kk] == 0) & (true_labels[kk] == 1):\n",
        "                        this_round_fns += 1\n",
        "\n",
        "                val_acc = accuracy_score(y_test, y_pred)\n",
        "                this_round_accuracies.append(val_acc)\n",
        "\n",
        "                X_train = np.delete(X_train, query_idx, axis=0)\n",
        "                y_train = np.delete(y_train, query_idx, axis=0)\n",
        "                print(f\"Fold no {fold+1} . Round no {round+1} . Query no {idx+1}: Acc = {val_acc:.2f}\")\n",
        "            rounds_accuracies.append(this_round_accuracies)\n",
        "            rounds_fps.append(this_round_fps)\n",
        "            rounds_fns.append(this_round_fns)\n",
        "        all_rounds_mean_accuracy = np.mean(rounds_accuracies, axis=0)\n",
        "        all_fold_acc_history.append(all_rounds_mean_accuracy)\n",
        "\n",
        "        all_rounds_fps = np.mean(rounds_fps)\n",
        "        all_rounds_fns = np.mean(rounds_fns)\n",
        "        all_fold_false_positives.append(all_rounds_fps)\n",
        "        all_fold_false_negatives.append(all_rounds_fns)\n",
        "\n",
        "        print(f\"Fold no {fold+1} . ALL ACC = {all_rounds_mean_accuracy}: total:{len(y_test)} : fps:{all_fold_false_positives} : nps:{all_fold_false_negatives}\")\n",
        "        timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "        with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/model_fold'+str(fold+1)+'-'+timestamp+'.pickle', 'wb') as handle:\n",
        "            pickle.dump(classifier, handle)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return  classifier, all_fold_acc_history , execution_time, all_fold_false_positives, all_fold_false_negatives\n",
        "\n"
      ],
      "metadata": {
        "id": "IgUpDh96BzBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_query_strategy(classifier, X, n_instances=1):\n",
        "    indices = list(range(len(X)))\n",
        "    random.shuffle(indices)\n",
        "    return indices[:n_instances]\n",
        "\n",
        "#random_query_strategy\n",
        "#margin_sampling\n",
        "#entropy_sampling\n",
        "#uncertainty_sampling\n"
      ],
      "metadata": {
        "id": "RtBB30ZcF5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title margin sampling\n",
        "import pickle\n",
        "\n",
        "STRATEGY = margin_sampling\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 100\n",
        "MAX_EPOCHS = 50\n",
        "FRACTION = 0.6\n",
        "np.random.seed(75)  # Set the seed value to 42\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 8,14,25))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))\n",
        "\n",
        "classifier_1, all_fold_acc_history_1, execution_time_1, fps_1,fns_1= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_1)\n",
        "print(fns_1)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(1, len(mean_accuracy_1) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "plt.errorbar(epochs_1, mean_accuracy_1, yerr=std_error_1, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_1, mean_accuracy_1, label='Average Accuracy')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_1, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'margin_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_1}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_1,'fps' : fps_1, 'fns': fns_1}, handle)\n"
      ],
      "metadata": {
        "id": "TuE95IrVEaV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title random sampling\n",
        "STRATEGY = random_query_strategy\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 100\n",
        "MAX_EPOCHS = 20\n",
        "FRACTION = 0.55\n",
        "np.random.seed(30)  # Set the seed value to 42\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 8,14,25))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))\n",
        "\n",
        "classifier_2, all_fold_acc_history_2, execution_time_2, fps_2,fns_2= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_2)\n",
        "print(fns_2)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(1, len(mean_accuracy_2) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.errorbar(epochs_2, mean_accuracy_2, yerr=std_error_2, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_2, mean_accuracy_2, label='Average Accuracy')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_2, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'random_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_2}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Mario-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_2,'fps' : fps_2, 'fns': fns_2}, handle)"
      ],
      "metadata": {
        "id": "o7dwIKmHDrcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "yROUoC91RqR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6yTvAhp1Ncj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(10, len(mean_accuracy_1) + 10)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(10, len(mean_accuracy_2) + 10)\n",
        "print(epochs_2)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.plot(epochs_1,  mean_accuracy_1, color='b', linestyle='-', label='Margin Sampling')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3)\n",
        "plt.plot(epochs_2,  mean_accuracy_2,color='r', linestyle='-', label='Random Sampling')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3)\n",
        "#plt.axhline(y=np.mean(all_fold_acc_history), color='g', linestyle='--', label='Passive Learner (2480 samples)')\n",
        "plt.ylim(0.45, 0.8)\n",
        "#plt.xlim(10, 410)\n",
        "#plt.xticks(np.arange(10,410, 50))\n",
        "plt.xlabel('#Samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Samples')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMkksMbCK0sKpBtyJMel2Pj"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}