{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "YXSETvJHR-lC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4vQoNfMCC-nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f967261c-6c91-47f2-e6ab-3f93633abd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/modAL-python/modAL.git\n",
            "  Cloning https://github.com/modAL-python/modAL.git to /tmp/pip-req-build-vgr5qic3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/modAL-python/modAL.git /tmp/pip-req-build-vgr5qic3\n",
            "  Resolved https://github.com/modAL-python/modAL.git to commit bba6f6fd00dbb862b1e09259b78caf6cffa2e755\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.25.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.11.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.5.3)\n",
            "Collecting skorch==0.9.0 (from modAL-python==0.4.2)\n",
            "  Downloading skorch-0.9.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch==0.9.0->modAL-python==0.4.2) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch==0.9.0->modAL-python==0.4.2) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->modAL-python==0.4.2) (1.16.0)\n",
            "Building wheels for collected packages: modAL-python\n",
            "  Building wheel for modAL-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for modAL-python: filename=modAL_python-0.4.2-py3-none-any.whl size=32654 sha256=d2b1de5cdb589369596f68c4d10aba4b0389872f1661ba3211e24b7f4570f52d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iu_zt4_0/wheels/d9/fb/59/7deb61b460c1c36394cd093758986ff7d36f71352dcb2e02c5\n",
            "Successfully built modAL-python\n",
            "Installing collected packages: skorch, modAL-python\n",
            "Successfully installed modAL-python-0.4.2 skorch-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/modAL-python/modAL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2kpfdZONDMni"
      },
      "outputs": [],
      "source": [
        "#@title import libraries\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pdb\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling\n",
        "from modAL.uncertainty import margin_sampling\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import Callback\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "szHmoSpJSM_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOYqtDtkwMTw",
        "outputId": "faf9bed0-0382-4f92-d3ca-beb20b146cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'#': 'ground', '-': 'empty', 'D': 'Door', 'H': 'Hazard', 'M': 'moving platform', 'T': 'platform'}\n"
          ]
        }
      ],
      "source": [
        "# Define the categories using a dictionary\n",
        "category_dict = {\"#\": \"ground\",\n",
        "                 \"-\": \"empty\",\n",
        "                 \"D\": \"Door\",\n",
        "                 \"H\": \"Hazard\",\n",
        "                 \"M\": \"moving platform\",\n",
        "                 \"T\": \"platform\"}\n",
        "\n",
        "def merge_dict_keys(dict_in):\n",
        "    # create a new dictionary with merged keys\n",
        "    dict_out = {}\n",
        "    for key, value in dict_in.items():\n",
        "        if value not in dict_out:\n",
        "            dict_out[key] = value\n",
        "        else:\n",
        "            dict_out[key] += ', ' + value\n",
        "\n",
        "    # remove redundant key-value pairs\n",
        "    dict_out = {v: k for k, v in dict_out.items()}\n",
        "    return {v: k for k, v in dict_out.items()}\n",
        "\n",
        "merged_category_dict = merge_dict_keys(category_dict)\n",
        "print(merged_category_dict)\n",
        "def create_matrix(string):\n",
        "    # Split the string into rows using newline characters.\n",
        "    rows = string.split('\\n')\n",
        "    # Remove any empty rows.\n",
        "    rows = [row for row in rows if row]\n",
        "    # Split each row into columns using individual characters.\n",
        "    matrix = []\n",
        "    for row in rows:\n",
        "        columns = [char for char in row]\n",
        "        matrix.append(columns)\n",
        "\n",
        "    # Return the resulting matrix.\n",
        "    return matrix\n",
        "def replace_characters(matrix):\n",
        "    # Define the characters to replace and their replacements.\n",
        "    replacements = {}\n",
        "    #  Create a new matrix to store the modified elements.\n",
        "    new_matrix = []\n",
        "    for row in matrix:\n",
        "        new_row = []\n",
        "        for element in row:\n",
        "            # If the current element is in the replacements dictionary, replace it.\n",
        "            if element in replacements:\n",
        "                new_row.append(replacements[element])\n",
        "            else:\n",
        "                new_row.append(element)\n",
        "        new_matrix.append(new_row)\n",
        "\n",
        "    # Return the modified matrix.\n",
        "    return new_matrix\n",
        "def create_binary_matrices(matrix):\n",
        "    # Define the characters to search for in the matrix.\n",
        "    characters = {'#', '-', 'D', 'H', 'T', 'M'}\n",
        "\n",
        "    # Determine the dimensions of the matrix.\n",
        "    num_rows = len(matrix)\n",
        "    num_cols = len(matrix[0])\n",
        "\n",
        "    # Create a dictionary to store the binary matrices.\n",
        "    binary_matrices = {}\n",
        "\n",
        "    # Loop through each character and create a binary matrix for that character.\n",
        "    for character in characters:\n",
        "        # Create a NumPy array to store the binary matrix.\n",
        "        binary_matrix = np.zeros((num_rows, num_cols))\n",
        "\n",
        "        # Loop through each element in the matrix and set the corresponding\n",
        "        # element in the binary matrix to 1 if it matches the current character.\n",
        "        for i in range(num_rows):\n",
        "            for j in range(num_cols):\n",
        "                if matrix[i][j] == character:\n",
        "                    binary_matrix[i][j] = 1\n",
        "\n",
        "        # Add the binary matrix to the dictionary.\n",
        "        binary_matrices[character] = binary_matrix\n",
        "\n",
        "    # Return the dictionary of binary matrices.\n",
        "    return binary_matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Resources"
      ],
      "metadata": {
        "id": "UGpe3InKSH6y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi-YnDEeDNNC",
        "outputId": "1fa767f2-c08a-4ead-89ef-bc73d71ff850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "gram_elite_info.json imported\n",
            "map_elites_info.json imported\n",
            "ngram_info.json imported\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "gram elite levels imported\n",
            "map elite levels imported\n",
            "n gram levels imported\n"
          ]
        }
      ],
      "source": [
        "# extracting labels\n",
        "drive.mount('/content/drive')\n",
        "gram_dict = {}\n",
        "map_dict = {}\n",
        "ngram_dict = {}\n",
        "\n",
        "# gram elite info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/icarus/gram_elites/generate_corpus_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    #print(data['fitness'].keys())\n",
        "    for filename in data['fitness'].keys():\n",
        "      gram_dict[filename] = data['fitness'][filename]\n",
        "print(\"gram_elite_info.json imported\")\n",
        "\n",
        "# map elite info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/icarus/map_elites/generate_corpus_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    for filename in data['fitness'].keys():\n",
        "      map_dict[filename] = data['fitness'][filename]\n",
        "print(\"map_elites_info.json imported\")\n",
        "\n",
        "# n gram info\n",
        "with open('/content/drive/MyDrive/Ghost Lab/levels/icarus/n_gram/generate_corpus_info.json') as f:\n",
        "    data = json.load(f)\n",
        "    for filename in data['fitness'].keys():\n",
        "      ngram_dict[filename] = data['fitness'][filename]\n",
        "print(\"ngram_info.json imported\")\n",
        "\n",
        "# extracting levels\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "playable_level_strings = []\n",
        "unplayable_level_strings = []\n",
        "playable_level_labels = []\n",
        "unplayable_level_labels = []\n",
        "\n",
        "# gram elite\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/icarus/gram_elites/levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/icarus/gram_elites/levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in gram_dict:\n",
        "            if gram_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"gram elite levels imported\")\n",
        "\n",
        "# map elite\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/icarus/map_elites/levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/icarus/map_elites/levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in map_dict:\n",
        "            if map_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"map elite levels imported\")\n",
        "\n",
        "# n gram\n",
        "for filename in os.listdir('/content/drive/MyDrive/Ghost Lab/levels/icarus/n_gram/levels'):\n",
        "  if filename.endswith('.txt'):\n",
        "      with open(os.path.join('/content/drive/MyDrive/Ghost Lab/levels/icarus/n_gram/levels', filename), 'r') as file:\n",
        "          file_contents = file.read()\n",
        "          if filename in ngram_dict:\n",
        "            if ngram_dict[filename] != 0:\n",
        "              playable_level_strings.append(file_contents)\n",
        "              playable_level_labels.append(1)\n",
        "            else:\n",
        "              unplayable_level_strings.append(file_contents)\n",
        "              unplayable_level_labels.append(0)\n",
        "print(\"n gram levels imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c4JsZ6ZiD0Jv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a20deb1-35f4-4685-c6a7-9889f9713b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of playable levels before balancing : 3798| number of unplayable levels before balancing : 2245|\n",
            "number of playable levels after balancing : 2245| number of unplayable levels after balancing : 2245|\n"
          ]
        }
      ],
      "source": [
        "#@title balance the dataset\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "#print(\"number of playable levels before balancing : %4d| number of unplayable levels before balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))\n",
        "# Reshape X to a 2D array\n",
        "#X_flat = np.reshape(levels, (levels.shape[0], -1))\n",
        "# Reshape y to a 2D array\n",
        "#y_flat = np.reshape(labels, (-1, 1))\n",
        "#oversampler = RandomUnderSampler()\n",
        "#X_resampled_flat, y_resampled_flat = oversampler.fit_resample(X_flat, y_flat)\n",
        "# Reshape the 2D array back to the original shape\n",
        "#X_resampled = np.reshape(X_resampled_flat, (-1, levels.shape[1], levels.shape[2], levels.shape[3]))\n",
        "# Reshape the y 2D array back to the original shape\n",
        "#y_resampled = np.reshape(y_resampled_flat, (-1,))\n",
        "\n",
        "#print(X_resampled.shape)\n",
        "#print(y_resampled.shape)\n",
        "#print(\"number of playable levels after balancing : %4d| number of unplayable levels after balancing : %4d|\" % (np.count_nonzero(y_resampled == 0), np.count_nonzero(y_resampled == 1)))\n",
        "print(\"number of playable levels before balancing : %4d| number of unplayable levels before balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))\n",
        "n = len(playable_level_strings) - len(unplayable_level_strings)\n",
        "for i in range(n):\n",
        "    index = random.randint(0, len(playable_level_strings)-1)\n",
        "    playable_level_strings.pop(index)\n",
        "    playable_level_labels.pop(index)\n",
        "print(\"number of playable levels after balancing : %4d| number of unplayable levels after balancing : %4d|\" % (len(playable_level_strings), len(unplayable_level_strings)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VZQtSH8nwZuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5537e9-53d2-44ae-a04c-a67de12d9681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2245\n",
            "2245\n",
            "the overal shape of X dataset: (4490, 6, 25, 16)\n",
            "the overal shape of Y dataset: (4490,)\n"
          ]
        }
      ],
      "source": [
        "#@title convert level strings to one-hot matrices\n",
        "print(len(playable_level_strings))\n",
        "print(len(unplayable_level_strings))\n",
        "level_strings = []\n",
        "level_strings.extend(playable_level_strings)\n",
        "level_strings.extend(unplayable_level_strings)\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "\n",
        "playable_idx = np.random.choice(range(len(playable_level_strings)), size=5, replace=False)\n",
        "unplayable_idx = np.random.choice(range(len(unplayable_level_strings)), size=5, replace=False)\n",
        "initial_x = []\n",
        "initial_y = []\n",
        "\n",
        "for i in playable_idx:\n",
        "  matrix = create_matrix(playable_level_strings[i])\n",
        "  replaced = replace_characters(matrix)\n",
        "  binary = create_binary_matrices(replaced)\n",
        "  initial_x.append(np.array(list(binary.values())))\n",
        "  initial_y.append(1)\n",
        "\n",
        "for i in playable_idx:\n",
        "  matrix = create_matrix(playable_level_strings[i])\n",
        "  replaced = replace_characters(matrix)\n",
        "  binary = create_binary_matrices(replaced)\n",
        "  initial_x.append(np.array(list(binary.values())))\n",
        "  initial_y.append(0)\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 6,25,16))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "-r6jc-8aSXmj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knyZ-o_CD8rX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(6, 16, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, padding=0)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(960, 32)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        #x = self.sigmoid(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passive Learner"
      ],
      "metadata": {
        "id": "6N4FvVgxSchc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "def encode_labels(labels):\n",
        "    if not isinstance(labels, torch.Tensor) or labels.dtype != torch.float32:\n",
        "        raise ValueError(\"Input must be a PyTorch tensor of float32 dtype.\")\n",
        "    if labels.ndim != 1 or not torch.all(torch.logical_or(labels == 0, labels == 1)):\n",
        "        raise ValueError(\"Input must be a 1D PyTorch tensor of 0s and 1s.\")\n",
        "    encoded_labels = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
        "    encoded_labels[torch.where(labels == 0)[0], 0] = 1\n",
        "    encoded_labels[torch.where(labels == 1)[0], 1] = 1\n",
        "    return encoded_labels\n",
        "\n",
        "X_tensor = torch.FloatTensor(levels)\n",
        "y_tensor = torch.FloatTensor(labels)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(train_data))\n",
        "\n",
        "num_folds = 5\n",
        "num_epochs = 50\n",
        "weight_decay=0.001\n",
        "learning_rate = 1e-2\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "all_fold_acc_history = []\n",
        "start_time = time.time()\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(test_data)):\n",
        "    model = Model3()\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "    train_fold_data = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_fold_data = torch.utils.data.Subset(test_data, val_indices)\n",
        "    train_fold_loader = torch.utils.data.DataLoader(train_fold_data, batch_size=32, shuffle=True)\n",
        "    val_fold_loader = torch.utils.data.DataLoader(val_fold_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    fold_acc_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for inputs, labels in train_fold_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, encode_labels(labels))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(val_fold_loader)\n",
        "        print(f\"Validation fold {fold + 1}, epoch {epoch + 1}: train_loss = {train_loss:.2f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            true_labels = []\n",
        "            pred_labels = []\n",
        "            for inputs, labels in val_fold_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, encode_labels(labels))\n",
        "                val_loss += loss.item()\n",
        "                y_pred = np.argmax(outputs, axis=1)\n",
        "                true_labels += labels.tolist()\n",
        "                pred_labels += y_pred.round().tolist()\n",
        "            val_loss /= len(val_fold_loader)\n",
        "\n",
        "        val_acc = accuracy_score(true_labels, pred_labels)\n",
        "        fold_acc_history.append(val_acc)\n",
        "        print(f\"Fold {fold + 1}: val_loss = {val_loss:.2f}, val_acc = {val_acc:.2f}\")\n",
        "    all_fold_acc_history.append(fold_acc_history)\n",
        "    all_true_labels += true_labels\n",
        "    all_pred_labels += pred_labels\n",
        "\n",
        "# Compute the overall confusion matrix and accuracy\n",
        "overall_cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "overall_acc = accuracy_score(all_true_labels, all_pred_labels)\n",
        "\n",
        "# Print the overall confusion matrix and accuracy\n",
        "print(f\"Overall confusion matrix:\\n{overall_cm}\")\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")\n",
        "\n",
        "# Compute the average accuracy over all folds for each epoch\n",
        "mean_acc_history = [sum([fold_acc_history[i] for fold_acc_history in all_fold_acc_history])/num_folds for i in range(len(all_fold_acc_history[0]))]\n",
        "\n",
        "# Plot the average accuracy over all folds over time\n",
        "plt.plot(mean_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Time')\n",
        "plt.show()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time of: {execution_time} seconds\")\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/passive/model_'+timestamp+'_.h5')\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'folds' : num_folds, 'epochs' : num_epochs,\n",
        "    'weight_decay' : weight_decay, 'learning_rate': learning_rate,\n",
        "    'execution_time' : execution_time}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/passive/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/passive/parameters-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(\n",
        "        {'all_fold_acc_history' : all_fold_acc_history,'confusion' : overall_cm}\n",
        "        , handle)"
      ],
      "metadata": {
        "id": "oeGRDuzZUaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learner"
      ],
      "metadata": {
        "id": "C_tnevySSlXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYcgdhToytEf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pdb\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def random_query_strategy(classifier, X, n_instances=1):\n",
        "    # Generate a list of indices to select random instances from X\n",
        "    indices = list(range(len(X)))\n",
        "    # Shuffle the indices to select random instances\n",
        "    random.shuffle(indices)\n",
        "    # Return the first n_instances instances from the shuffled indices\n",
        "    return indices[:n_instances]\n",
        "\n",
        "#random_query_strategy\n",
        "#margin_sampling\n",
        "#entropy_sampling\n",
        "#uncertainty_sampling\n",
        "\n",
        "def train_active_learner(strategy, n_queries, n_instances, max_epochs, fraction):\n",
        "    num_folds = 5\n",
        "    num_rounds = 5\n",
        "    all_fold_acc_history = []\n",
        "    all_fold_false_positives = []\n",
        "    all_fold_false_negatives = []\n",
        "    # initial training dataset\n",
        "    X_tensor = torch.FloatTensor(levels)\n",
        "    y_tensor = torch.LongTensor(labels)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(X_tensor).to(device), torch.tensor(y_tensor).to(device))\n",
        "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "    X , y = next(iter(dataset_loader))\n",
        "    X = X.detach().cpu().numpy()\n",
        "    y = y.detach().cpu().numpy()\n",
        "    #\n",
        "    X_ini = torch.FloatTensor(initial_x)\n",
        "    y_ini = torch.LongTensor(initial_y)\n",
        "    ini_dataset = torch.utils.data.TensorDataset(torch.tensor(X_ini).to(device), torch.tensor(y_ini).to(device))\n",
        "    ini_loader = torch.utils.data.DataLoader(ini_dataset, batch_size=len(ini_dataset))\n",
        "    X_0 , y_0 = next(iter(ini_loader))\n",
        "    X_0 = X_0.detach().cpu().numpy()\n",
        "    y_0 = y_0.detach().cpu().numpy()\n",
        "\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "        X_train = X[train_indices]\n",
        "        X_test= X[val_indices]\n",
        "        y_train = y[train_indices]\n",
        "        y_test = y[val_indices]\n",
        "\n",
        "        print(f\"Fold no {fold+1}\")\n",
        "        # 5 trials\n",
        "        rounds_accuracies = []\n",
        "        rounds_fps = []\n",
        "        rounds_fns = []\n",
        "        for round in range (0,num_rounds):\n",
        "            print(f\"Fold no {fold+1} . Round no {round+1}\")\n",
        "            this_round_accuracies = []\n",
        "            this_round_fps = 0\n",
        "            this_round_fns = 0\n",
        "            classifier = NeuralNetClassifier(Model, criterion=nn.CrossEntropyLoss,\n",
        "                                 optimizer=torch.optim.Adam, optimizer__weight_decay=0.001,\n",
        "                                 max_epochs = max_epochs, train_split=None,\n",
        "                                 verbose=0, device=device, warm_start = True,)\n",
        "            learner = ActiveLearner(estimator=classifier,X_training=X_0, y_training=y_0,query_strategy=strategy)\n",
        "            y_pred = learner.predict(X_test)\n",
        "            ini_acc = accuracy_score(y_test, y_pred)\n",
        "            print(ini_acc)\n",
        "            fold_acc_history = []\n",
        "            for idx in range(n_queries):\n",
        "                random_indices = np.random.choice(range(len(X_train)), size=math.floor(fraction*len(X_train)), replace=False)\n",
        "                query_idx, query_instance = learner.query(X_train[random_indices], n_instances=n_instances)\n",
        "                x_query = X_train[query_idx]\n",
        "                y_query = y_train[query_idx]\n",
        "                learner.teach(X=x_query, y=y_query)\n",
        "                y_pred = learner.predict(X_test)\n",
        "\n",
        "                true_labels = y_test.tolist()\n",
        "                pred_labels = y_pred.tolist()\n",
        "                for kk in range(len(true_labels)):\n",
        "                    if (pred_labels[kk] == 1) & (true_labels[kk] == 0):\n",
        "                        this_round_fps += 1\n",
        "                    if (pred_labels[kk] == 0) & (true_labels[kk] == 1):\n",
        "                        this_round_fns += 1\n",
        "\n",
        "                val_acc = accuracy_score(y_test, y_pred)\n",
        "                this_round_accuracies.append(val_acc)\n",
        "\n",
        "                X_train = np.delete(X_train, query_idx, axis=0)\n",
        "                y_train = np.delete(y_train, query_idx, axis=0)\n",
        "                print(f\"Fold no {fold+1} . Round no {round+1} . Query no {idx+1}: Acc = {val_acc:.2f}\")\n",
        "            rounds_accuracies.append(this_round_accuracies)\n",
        "            rounds_fps.append(this_round_fps)\n",
        "            rounds_fns.append(this_round_fns)\n",
        "        all_rounds_mean_accuracy = np.mean(rounds_accuracies, axis=0)\n",
        "        all_fold_acc_history.append(all_rounds_mean_accuracy)\n",
        "\n",
        "        all_rounds_fps = np.mean(rounds_fps)\n",
        "        all_rounds_fns = np.mean(rounds_fns)\n",
        "        all_fold_false_positives.append(all_rounds_fps)\n",
        "        all_fold_false_negatives.append(all_rounds_fns)\n",
        "\n",
        "        print(f\"Fold no {fold+1} . ALL ACC = {all_rounds_mean_accuracy}: total:{len(y_test)} : fps:{all_fold_false_positives} : nps:{all_fold_false_negatives}\")\n",
        "        timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "        with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/model_fold'+str(fold+1)+'-'+timestamp+'.pickle', 'wb') as handle:\n",
        "            pickle.dump(classifier, handle)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return  classifier, all_fold_acc_history , execution_time, all_fold_false_positives, all_fold_false_negatives\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nPKDgUhyvWF"
      },
      "outputs": [],
      "source": [
        "#@title Margin Sampling\n",
        "import pickle\n",
        "\n",
        "STRATEGY = margin_sampling\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 400\n",
        "MAX_EPOCHS = 50\n",
        "np.random.seed(15)  # Set the seed value to 42\n",
        "FRACTION = 0.5\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 6,25,16))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))\n",
        "\n",
        "classifier_1, all_fold_acc_history_1, execution_time_1, fps_1,fns_1= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_1)\n",
        "print(fns_1)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(1, len(mean_accuracy_1) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "plt.errorbar(epochs_1, mean_accuracy_1, yerr=std_error_1, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_1, mean_accuracy_1, label='Average Accuracy')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_1, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'margin_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_1}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_1,'fps' : fps_1, 'fns': fns_1}, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzXCZQKty3yi"
      },
      "outputs": [],
      "source": [
        "#@title random sampling\n",
        "STRATEGY = random_query_strategy\n",
        "N_INSTANCES = 1\n",
        "N_QUERIES = 100\n",
        "MAX_EPOCHS = 50\n",
        "FRACTION = 0.5\n",
        "np.random.seed(30)  # Set the seed value to 42\n",
        "\n",
        "# Define the size of the level\n",
        "width = 29\n",
        "height = 13\n",
        "num_levels = len(level_strings)\n",
        "levels = np.zeros((num_levels, 8,14,25))\n",
        "\n",
        "for i, level_string in enumerate(level_strings):\n",
        "    matrix = create_matrix(level_string)\n",
        "    replaced = replace_characters(matrix)\n",
        "    binary = create_binary_matrices(replaced)\n",
        "    levels[i] = np.array(list(binary.values()))\n",
        "labels = []\n",
        "labels.extend(playable_level_labels)\n",
        "labels.extend(unplayable_level_labels)\n",
        "labels = np.array(labels)\n",
        "print('the overal shape of X dataset: ' + str(levels.shape))\n",
        "print('the overal shape of Y dataset: ' + str(labels.shape))\n",
        "\n",
        "classifier_2, all_fold_acc_history_2, execution_time_2, fps_2,fns_2= train_active_learner(STRATEGY, N_QUERIES, N_INSTANCES,MAX_EPOCHS, FRACTION)\n",
        "\n",
        "print(f\"Execution time: {execution_time_1} seconds\")\n",
        "\n",
        "print(fps_2)\n",
        "print(fns_2)\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(1, len(mean_accuracy_2) + 1)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.errorbar(epochs_2, mean_accuracy_2, yerr=std_error_2, capsize=3)\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Epoch with Standard Error')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_2, mean_accuracy_2, label='Average Accuracy')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3, label='Standard Deviation')\n",
        "plt.xlabel('Queries')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Queries with Standard Deviation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = str(int(time.time()))  # Obtain the current timestamp\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/model_'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump(classifier_2, handle)\n",
        "\n",
        "json_object = json.dumps({\n",
        "    'STRATEGY' : 'random_sampling', 'num_instances' : N_INSTANCES,\n",
        "    'num_queries' : N_QUERIES, 'epochs' : MAX_EPOCHS,\n",
        "    'fraction': FRACTION,\n",
        "    'weight_decay' : 0.001, 'learning_rate': '?',\n",
        "    'execution_time' : execution_time_2}, indent=4)\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/parameters-'+timestamp+'.json', 'w') as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ghost Lab/cog 2023/camera-ready/Icarus-data/active/results-'+timestamp+'.pickle', 'wb') as handle:\n",
        "    pickle.dump({'all_fold_acc_history' : all_fold_acc_history_2,'fps' : fps_2, 'fns': fns_2}, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "VxvxKrSNS259"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_1 = np.mean(all_fold_acc_history_1, axis=0)\n",
        "std_error_1 = np.std(all_fold_acc_history_1, axis=0) / np.sqrt(len(all_fold_acc_history_1))\n",
        "std_dev_1 = np.std(all_fold_acc_history_1, axis=0)\n",
        "# Plotting\n",
        "epochs_1 = range(10, len(mean_accuracy_1) + 10)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_1 = mean_accuracy_1 - std_dev_1\n",
        "upper_bound_1 = mean_accuracy_1 + std_dev_1\n",
        "\n",
        "\n",
        "# Calculate average accuracy and standard error for each epoch\n",
        "mean_accuracy_2 = np.mean(all_fold_acc_history_2, axis=0)\n",
        "\n",
        "std_error_2 = np.std(all_fold_acc_history_2, axis=0) / np.sqrt(len(all_fold_acc_history_2))\n",
        "std_dev_2 = np.std(all_fold_acc_history_2, axis=0)\n",
        "# Plotting\n",
        "epochs_2 = range(10, len(mean_accuracy_2) + 10)\n",
        "print(epochs_2)\n",
        "# Calculate upper and lower bounds for the fill region\n",
        "lower_bound_2 = mean_accuracy_2 - std_dev_2\n",
        "upper_bound_2 = mean_accuracy_2 + std_dev_2\n",
        "\n",
        "plt.plot(epochs_1,  mean_accuracy_1, color='b', linestyle='-', label='Margin Sampling')\n",
        "plt.fill_between(epochs_1, lower_bound_1, upper_bound_1, alpha=0.3)\n",
        "plt.plot(epochs_2,  mean_accuracy_2,color='r', linestyle='-', label='Random Sampling')\n",
        "plt.fill_between(epochs_2, lower_bound_2, upper_bound_2, alpha=0.3)\n",
        "#plt.axhline(y=np.mean(all_fold_acc_history), color='g', linestyle='--', label='Passive Learner (2480 samples)')\n",
        "plt.ylim(0.45, 0.8)\n",
        "#plt.xlim(10, 410)\n",
        "#plt.xticks(np.arange(10,410, 50))\n",
        "plt.xlabel('#Samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Accuracy over Samples')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ArjM7EOOrbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8YNNvEZHeW7dgCyLf0pLn"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}